---
layout: post
title: "Towards Scene Understanding: Unsupervised Monocular Depth Estimation with Semantic-aware Representation"
tags: [Deep Learning, Depth Estimation, Semantic Segmentation]
comments: true
---

오늘 소개하고자 하는 논문은 CVPR 2019에 accept된 "Towards Scene Understanding" 이라는 논문이다.

기본적인 아이디어는 다음과 같다. 지금까지 진행해왔던 Depth Estimation은 크게 두 분류 (Supervised, Unsupervised)로 나눌수 있다.
Supervised learning에서의 가장 큰 한계점은 믿을 수 있는 ground truth 데이터를 구하기가 expensive하고 time-consuming하다는 점이다. 
이러한 한계점을 개선해보고자 unsupervsied learning은 stereo vision의 아이디어를 도입하여 depth estimation을 진행했다. 
Stereo pair를 사용함으로서 object의 geometric structure를 학습할 수 있고 그렇게 되면 projection 되는 장면의 3D 정보를 구할 수 있게 된다.
Stereo pair로 training을 시키고 inference 단계에서는 single image로 만으로 depth estimation 을 진행 할 수 있게 된다. 

이 논문에서는 다음과 같은 unsupervised learning을 사용한 depth estimation의 성능을 좀 더 올려보고자 semantic segmentation을 병렬적으로 수행한다.
Semantic segmentation branch에서 나온 결과를 토대로 semantic understanding을 구하고 depth estimation branch에서 나온 결과로 geometric understanding을 구할수 있다.
이러한 두 가지의 understanding을 합하여 만든 scene representation을 학습시킴으로써 depth estimation의 성능이 좀 더 올라간다고 한다.

좀  더 자세히 알아보자.

## Intro
저자가 reference로 삼는 논문은 [Monodepth](https://)이라고 하는 Godard라는 저자가 2017년 cvpr에 작성했던 논문이다. Monodepth의 기본적인 아이디어는 
전 포스트 중에 있으니 그걸 참고하길 바란다. 이 논문에서 말하는 Monodepth의 가장 큰 문제점은 하나의 이미지로 stereo view에 대한 disparity를 예측하는건 
'mismatching'이라는 문제점을 초래한다고한다. 만약 stereo pair에서 왼쪽 이미지를 기반으로 왼쪽과 오른쪽 view에 대한 disparity 들을 둘다 예측하게 training을 시킨다면,
inference 단계에서는 오른쪽 view의 structure 정보를 잘 모르기 때문에 부정확한 결과값이 나올 가능성이 크다는 것이다.

그래서 이러한 부정확성을 여기 SceneNet에서는 밑에 보이는 사진에서와 같이 scene understanding branch를 추가해줌으로써 해결하고자한다. 
<img src="https://github.com/abeyang00/abeyang00.github.io/blob/master/assets/img/monoResMatch_architecture.png">
SceneNet은 기본적인 encoder-decoder 기반의 network이다. Input으로 들어오는 이미지를 encoder를 통해 representations으로 인코딩해주고 decoder를 통해
depth 아니면 segmentation 결과를 구할 수 있게 된다. Decoder를 multi-task classifier라고 볼수 있지만 두 다른 task에 대한 paramemter들은 공유한다. 
논문에서는 'Decoder acts as a multi-task yet shared classifier that transforms scene representation into the prediction of depth or segmentation'이라고 설명한다.

나중에 설명을 다시 하겠지만 이렇게 'multi-task yet shared classifier'는 task identity라는 mechanism 을 조건으로 만들어진다. 그래서 결과적으로 
SceneNet은 depth와 segmentation를 bonding하는 cross-modal network model이라고 볼수 있다. 





### Multi-scale feature extractor

  
### Initial Disparity Estimation

### Disparity Refinement


