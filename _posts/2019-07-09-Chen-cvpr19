---
layout: post
title: "Towards Scene Understanding: Unsupervised Monocular Depth Estimation with Semantic-aware Representation"
tags: [Deep Learning, Depth Estimation, Semantic Segmentation]
comments: true
---

오늘 소개하고자 하는 논문은 CVPR 2019에 accept된 "Towards Scene Understanding" 이라는 논문이다.

기본적인 아이디어는 다음과 같다. 지금까지 진행해왔던 Depth Estimation은 크게 두 분류 (Supervised, Unsupervised)로 나눌수 있다.
Supervised learning에서의 가장 큰 한계점은 믿을 수 있는 ground truth 데이터를 구하기가 expensive하고 time-consuming하다는 점이다. 
이러한 한계점을 개선해보고자 unsupervsied learning은 stereo vision의 아이디어를 도입하여 depth estimation을 진행했다. 
Stereo pair를 사용함으로서 object의 geometric structure를 학습할 수 있고 그렇게 되면 projection 되는 장면의 3D 정보를 구할 수 있게 된다.
Stereo pair로 training을 시키고 inference 단계에서는 single image로 만으로 depth estimation 을 진행 할 수 있게 된다. 

이 논문에서는 다음과 같은 unsupervised learning을 사용한 depth estimation의 성능을 좀 더 올려보고자 semantic segmentation을 병렬적으로 수행한다.
Semantic segmentation branch에서 나온 결과를 토대로 semantic understanding을 구하고 depth estimation branch에서 나온 결과로 geometric understanding을 구할수 있다.
이러한 두 가지의 understanding을 합하여 만든 scene representation을 학습시킴으로써 depth estimation의 성능이 좀 더 올라간다고 한다.

좀  더 자세히 알아보자.

## Intro


## Monocular Residual Matching (monoResMatch)
MonoResMatch는 single image가 input으로 들어오게 되면 self-supervised manner로 정확하고 dense한 depth estimation을 하는 architecture이다. 이 네트워크는 3가지의 요소로 나눌수 있다. 
1. Multi-scale feature extractor: single image가 카메라로 부터 input으로 들어오게 되면 다양한 scales (quarter resolution부터 full resolution)의 representations들을 계산한다. 
2. Initial Disparity Estimation: Multi-scale feature extractor에서 계산된 multi-scale feature representations들을 기반으로 각 scale에서 disparity map을 추출해낸다.
3. Disparity Refinement: real representation과 synthesized된 representation 사이에서 stereo matching을 수행한다. 

위에 설명한 3가지 요소들은 single architecture안에 구성되어 있으며 end-to-end로 training이 진행된다.

monoResMatch의 architecture는 다음과 같다.
<img src="https://github.com/abeyang00/abeyang00.github.io/blob/master/assets/img/monoResMatch_architecture.png">

이제 3가지의 요소를 좀더 자세히 알아보자.

### Multi-scale feature extractor
'Learning for disparity estimation through feature constancy'라는 논문에서 영감을 받아 single image가 인풋으로 들어오게 되면 여러개의 convolutional layers를 통해 multi-scale representations를 계산한다.
- 먼저 두개의 convolution layers를 지나게 된다. 첫번째 conv layer 는 64개의 7x7 사이즈의 filter를 stride=2로 지정하여 convolution 을 진행한다. 
  두번째 conv layer에서는 128개의 4x4 filter를 stride=2로 지정하여 convolution을 진행한다. 
- 그 다음은 앞에서 구해진 2개의 feature map을 다시 원본 사이즈로 upsample 시킨다.
- 마지막으로 upsample된 두개의 feature map을 concat 시키고 1x1 conv layer를 통해 final representation을 구하게 된다. 
결과적으로 2개의 다른 scale의 representations과 그 두개의 representations을 섞은 또 다른 representation을 ouput으로 가지게 된다.
  
### Initial Disparity Estimation
이 모듈은 앞서 'Multi-scale feature extractor' 모듈에서 구해진 multi-scale representations들을 기반으로 multi-scale ($$\frac{1}{128}$$ ~ full resolution) disparity maps들을 구하게 된다. 'A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation'에서 나온 DispNet라는 encoder-decoder를 backbone architecture로 갖는다.
- Encoder에서 각 down-sampling module은 1과 2의 stride를 갖는 2개의 conv blocks로 이루어져있다. 
  - 각 conv block에서는 3x3 사이즈의 filter를 사용하고, 사용되는 filter의 갯수를 점점 증가시킨다. (64 -> 128 -> 256 -> 512 ->1024)
  - ReLU를 non-linear activation function으로 선정하였다. 
- Decoder에서는 skip-connection을 사용해 upsampling을 진행하여 2개의 disparity maps를 output으로 갖는다.
  - 첫번째 ouput disparity map은 original input frame과 align된 disparity map
  - 두번째는 output은 'Unsupervised Monocular Depth Estimation with Left-Right Consistency'의 아이디어를 사용해 input image 의 오른쪽 가상시점 ('virtual viewpoint on its right')과 align된 disparity map
  - 각 multi-scale에 대해 위와 같이 두개의 disparity map을 추출해낸다.
이렇게 구해진 initial disparity maps들을 discontinuities와 occluded regions들에 대해 취약한 성능을 보인다. 취약한 부분을 매꾸기 위해 그 다음 step인 Disparity refinement를 진행한다.
  
### Disparity Refinement
이 모듈에서는 Initial Disparity Estimation에서 구해진 multi-scale disparity maps를 기반으로 진행된다.


