---
title: Unsupervised Monocular Depth Estimation with Left-Right Consistency
subtitle: cvpr 2017
tags: [Deep Learning, Depth Estimation]
---

CVPR 2017년에 accept 된 논문으로 지금까지 supervised 방식으로 진행해왔던 depth estimation을 unsupervised learning으로 진행하고, 오히려 supervised쪽에서의 state-of-the-art 알고리즘들보다 압도적인 성능결과를 얻은 논문이다.

이 논문이 나오기 전까지 depth estimation은 supervised regression 문제로 해결하려고 했었다. 엄청 많은 양의 depth ground truth가 있다고 가정을 하고 regression network에서 나오는 output을 간단한 pixel-level loss (L1, L2, MSE, 등등...)를 minimize하는 형식으로 진행하였다.

Network를 다양한 환경에서 generalize 시킬려면 엄청나게 많은 ground truth 데이터가 필요하다. 다들 알다시피 depth ground truth는 구하기가 굉장히 어렵기 때문에 이 논문에서는 ground truth의 필요성을 아예 없애버린다. Ground truth를 사용하지 않고 binocular stereo 영상을 사용하여 unsupervised형식으로 depth estimation을 진행하게된다. 자세한 설명은 후에 또 하겠지만, 간단하게 말하면 Epipolar geometry constraints (epipolar geometry에 대해 자세히 알고싶다면 [다크프로그래머를 통해](https://darkpgmr.tistory.com/83?category=460965))를 사용하여 disparity를 구하고, image reconstruction loss를 사용하여 network loss를 minimize시켜준다. 하지만 image reconstruction loss만을 사용하게 되면 결과로 나온 disparity가 blur하고 정확성이 떨어지기 때문에 stereo pair (left과 right 이미지)의 consistency를 고려해 loss에 추가해주게 된다.

이 논문에서 직접 말하는 contributions들은
1. left-right depth consistency를 고려해서 새로만든 loss function으로 network를 end-to-end 로 training을 할 수 있다는점
2. 다양한 training loss들과 비교했을때 이 논문의 loss function이 월등하다는 점 (뭐 이건 1번이랑 비슷한 맥락)
3. 자기들이 새로 취득한 데이터를 포함한 3가지 새로운 dataset에도 generalization이 잘된다는 점 
4. Inference시간은 35 milisecond 밖에 걸리지않아 real-time application에 적용할 수 있다는 점

이제 논문에서 제시한 방법론을 자세히 알아보고자 한다.
### Method
#### Depth Estimation as Image Reconstruction
이 네트워크의 목표는 카메라로부터 single image $$I$$가 들어왔을 때 depth map $$\hat{d}=f(I)$$를 구할 수 있는 mapping function $$f$$를 구하는 것이다. 앞서 말했듯 ground truth를 사용하지 않고 depth estimation을 image reconstruction 문제로 바꾼다. Calibration이 되어있는 stereo image pair ($$I^l,I^r$$ l:left, r:right)가 있다고 가정을 하면, 하나의 이미지 $$I^l$$로부터 $$I^r$$로 reconstruction할 수 있는 function을 학습시킬 수 있다면, 우리는 projection되는 scene에 대한 3차원 정보를 얻을 수 있다. 그래서 이렇게 구한 3차원 정보로 이 논문에서는 depth estimation을 진행한다. 

Training에서는 network가 depth를 바로 구하는 것이 아니라 먼저 correspondence field ($$d^r$$, $$d^l$$)를 구한다. left 이미지가 input 으로 들어갔을땐 right correspondence field $$d^r$$을, right 이미지가 input으로 들어갔을땐 left correspondence field $$d^l$$을 구하게 된다. 이렇게 구해진 correspondence field로 left 이미지에서는 right 이미지를 $$\tilde{I^r}=I^l(d^r)$$, right 이미지에서는 left 이미지를 reconstruction $$\tilde{I^l}=I^r(d^l)$$ 시킨다. 

결론적으로 $$d$$를 network에서 구한 disparity라고 부른다. 두 카메라간의 baseline distance $$b$$와 inference때 사용할 카메라의 focal length $$f$$를 알고 있다면, depth $$\hat{d} = \frac{bf}{d}$$ 라는 식으로 depth를 recover 할 수 있다.

#### Depth Estimation Network
이 논문의 key insight는 left input 이미지만 사용을 하여 동시에 두개의 disparity maps (left-to-right과 right-to-left)를 추론하고 두 disparities를 서로에게 consistent하게 강요함으로서 좀 더 좋은 결과를 얻는것이다.




