---
layout: post
title: "Learning monocular depth estimation infusing traditional stereo knowledge"
tags: [Deep Learning, Depth Estimation]
comments: true
---

CVPR 2019에 accept된 "Learning monocular depth estimation infusing traditional stereo knowledge" 이라는 논문을 알아보고자 한다. 

카메라를 사용해 depth estimation을 진행하는 분야에서는 stereo(2개 이상의 카메라를 사용하는)가 disparity를 추론하는데 선호된다. Single camera를 사용하는 것 보다 당연하게 더 좋은 성능을 선보이지만 stereo를 사용하게 된다면 많은 pre-processing 단계가 필요하다. Stereo 장비들이 필요한것 외에도 굉장히 정확한 calibration 값들이 필요하다. 그렇다면 stereo의 정보를 더해서 네트워크를 train하고 inference 단계에서는 single 이미지로 depth estimation을 수행할 수 있을까? 

이 논문에서는 stereo matching 의 장점을 활용해 monocular depth estimation의 성능을 높이고자한다. 이 목적을 위해 monoResMatch라는 새로운 architecture를 제시한다. 다양한/다른 시점에서, input 이미지와 수평으로 정렬 (horizontally align) 된 features를 합성하고, 두개의 cue사이에서 stereo matching을 수행한다. 또한 Semi-Global Matching 같은 traditional stereo 알고리즘을 통해 대리 (proxy) ground truth를 얻는 것에 대한 장점도 설명하고 있다. Proxy ground truth를 사용함으로써 (1) self-supervised 방식을 유지하고 (2) expensive한 depth labels에 대한 필요성을 대처하며 (3) 보다 정확한 monocular depth estimation이 가능하다는 것을 보여준다. 구해진 proxy ground truth에는 당연히 이상한 값들이 들어있는 outliers들이 존재한다. 그래도 이 paradigm을 사용하여 training을 하게되면 월등히 좋은 성능을 내보낸다고 한다.

지금부터 이 논문이 제안하는 네트워크에 대해 좀 더 자세히 알아보자.

## Monocular Residual Matching (monoResMatch)
MonoResMatch는 single image가 input으로 들어오게 되면 self-supervised manner로 정확하고 dense한 depth estimation을 하는 architecture이다. 이 네트워크는 3가지의 요소로 나눌수 있다. 
1. Multi-scale feature extractor: single image가 카메라로 부터 input으로 들어오게 되면 다양한 scales (quarter resolution부터 full resolution)의 representations들을 계산한다. 
2. Initial Disparity Estimation: Multi-scale feature extractor에서 계산된 multi-scale feature representations들을 기반으로 각 scale에서 disparity map을 추출해낸다.
3. Disparity Refinement: real representation과 synthesized된 representation 사이에서 stereo matching을 수행한다. 

위에 설명한 3가지 요소들은 single architecture안에 구성되어 있으며 end-to-end로 training이 진행된다.

monoResMatch의 architecture는 다음과 같다.
<img src="https://github.com/abeyang00/abeyang00.github.io/blob/master/assets/img/monoResMatch_architecture.png">

이제 3가지의 요소를 좀더 자세히 알아보자.

### Multi-scale feature extractor
'Learning for disparity estimation through feature constancy'라는 논문에서 영감을 받아 single image가 인풋으로 들어오게 되면 여러개의 convolutional layers를 통해 multi-scale representations를 계산한다.
- 먼저 두개의 convolution layers를 지나게 된다. 첫번째 conv layer 는 64개의 7x7 사이즈의 filter를 stride=2로 지정하여 convolution 을 진행한다. 
  두번째 conv layer에서는 128개의 4x4 filter를 stride=2로 지정하여 convolution을 진행한다. 
- 그 다음은 앞에서 구해진 2개의 feature map을 다시 원본 사이즈로 upsample 시킨다.
- 마지막으로 upsample된 두개의 feature map을 concat 시키고 1x1 conv layer를 통해 final representation을 구하게 된다. 
결과적으로 2개의 다른 scale의 representations과 그 두개의 representations을 섞은 또 다른 representation을 ouput으로 가지게 된다.
  
### Initial Disparity Estimation
이 모듈은 앞서 'Multi-scale feature extractor' 모듈에서 구해진 multi-scale representations들을 기반으로 multi-scale ($$\frac{1}{128}$$ ~ full resolution) disparity maps들을 구하게 된다. 'A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation'에서 나온 DispNet라는 encoder-decoder를 backbone architecture로 갖는다.
- Encoder에서 각 down-sampling module은 1과 2의 stride를 갖는 2개의 conv blocks로 이루어져있다. 
  - 각 conv block에서는 3x3 사이즈의 filter를 사용하고, 사용되는 filter의 갯수를 점점 증가시킨다. (64 -> 128 -> 256 -> 512 ->1024)
  - ReLU를 non-linear activation function으로 선정하였다. 
- Decoder에서는 skip-connection을 사용해 upsampling을 진행하여 2개의 disparity maps를 output으로 갖는다.
  - 첫번째 ouput disparity map은 original input frame과 align된 disparity map
  - 두번째는 output은 'Unsupervised Monocular Depth Estimation with Left-Right Consistency'의 아이디어를 사용해 input image 의 오른쪽 가상시점 ('virtual viewpoint on its right')과 align된 disparity map
  - 각 multi-scale에 대해 위와 같이 두개의 disparity map을 추출해낸다.
이렇게 구해진 initial disparity maps들을 discontinuities와 occluded regions들에 대해 취약한 성능을 보인다. 취약한 부분을 매꾸기 위해 그 다음 step인 Disparity refinement를 진행한다.
  
### Disparity Refinement
이 모듈에서는 Initial Disparity Estimation에서 구해진 multi-scale disparity maps를 기반으로 진행된다.



